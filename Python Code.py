# -*- coding: utf-8 -*-
"""BTPMLUpdated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14iOfJ6-EYgoFGYwBC_rNBH7CxhN4EgqX
"""

import pandas as pd
import numpy as np

try:
    from thefuzz import process, fuzz
except ImportError:
    !pip install thefuzz[speedup]
    from thefuzz import process, fuzz


# Load the datasets
# dataset1: Contains a column 'Funds' with mutual fund names
df1 = pd.read_csv('Mutual Fund Dataset 2025.csv')

# dataset2: Contains 'scheme names' and 'scheme codes'
df2 = pd.read_csv('Indian_growth_direct_mfs_mapping_id_to_name.csv')

# Create a new column in df1 to store the matched scheme code
df1['Scheme Code'] = None

# Prepare a list of scheme names from df2 for fuzzy matching
scheme_names_list = df2['Scheme Name'].tolist()

# Set a threshold for a "good" match; adjust if needed
threshold = 80

# Iterate over each mutual fund in df1
for idx, row in df1.iterrows():
    fund_name = row['Funds']

    # Use fuzzy matching to find the best matching scheme name
    result = process.extractOne(fund_name, scheme_names_list, scorer=fuzz.token_set_ratio)

    if result is None:
        df1.at[idx, 'Scheme Code'] = None
        continue

    # Check if the result has an index (length 3) or not (length 2)
    if len(result) == 3:
        best_match, score, match_index = result
    else:
        best_match, score = result
        match_index = scheme_names_list.index(best_match)

    if score >= threshold:
        scheme_code = df2.iloc[match_index]['Scheme Code']
        df1.at[idx, 'Scheme Code'] = scheme_code
    else:
        df1.at[idx, 'Scheme Code'] = None

# Display the updated DataFrame
df1.head()
df1=df1.dropna(subset=['Scheme Code'])
df1['Scheme Code']=df1['Scheme Code'].astype(int)
df1

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Replace missing return values ('-') with NaN and convert to float
return_cols = [col for col in df1.columns if 'Return (%)' in col]
df1[return_cols] = df1[return_cols].replace('-', np.nan).astype(float)

# Fill or drop missing values ‚Äî here we fill with median
df1[return_cols] = df1[return_cols].fillna(df1[return_cols].median())

# Convert AUM and Expense Ratio to float (remove commas or other symbols if needed)
# Replace '-' with NaN first
df1['AUM(in Rs. cr)'] = df1['AUM(in Rs. cr)'].replace('-', np.nan)
# Remove commas if any, convert to float
df1['AUM(in Rs. cr)'] = df1['AUM(in Rs. cr)'].astype(str).str.replace(',', '', regex=False).astype(float)
# Fill NaNs with median
df1['AUM(in Rs. cr)'] = df1['AUM(in Rs. cr)'].fillna(df1['AUM(in Rs. cr)'].median())

df1['ExpenseRatio (%)'] = df1['ExpenseRatio (%)'].replace('-', np.nan).astype(float)
df1['ExpenseRatio (%)'] = df1['ExpenseRatio (%)'].fillna(df1['ExpenseRatio (%)'].median())

# Select numerical features
numerical_cols = ['AUM(in Rs. cr)', 'ExpenseRatio (%)'] + return_cols
X_num = df1[numerical_cols].values

# --- NLP-style Category Encoding ---
# Clean and use TF-IDF on the 'Category' column
df1['Category'] = df1['Category'].astype(str)
vectorizer = TfidfVectorizer()
X_cat = vectorizer.fit_transform(df1['Category']).toarray()
cat_feature_names = vectorizer.get_feature_names_out()

# Combine numerical + category features
X_combined = np.hstack([X_num, X_cat])
print(X_combined.shape)
print(type(X_combined))
print(np.isnan(X_combined).sum())  # Should be 0

# Scale the combined features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_combined)

# --- Clustering ---
k = 2  # Try different values of k and use the elbow method if needed
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_scaled)
df1['Cluster'] = clusters

# --- Visualization using PCA ---
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(12, 8))
palette = sns.color_palette('tab10', n_colors=k)
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df1['Cluster'], palette=palette)

# Add text labels for each point with its Category
for i, (x, y) in enumerate(zip(X_pca[:, 0], X_pca[:, 1])):
  if i%4==0:
    plt.text(x + 0.01, y + 0.01, df1['Category'].iloc[i][0], fontsize=8, alpha=0.8)

plt.title('Fund Clusters with Category Labels')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title='Cluster')
plt.grid(True)
plt.tight_layout()
plt.show()


# cluster_summary = df1.groupby('Cluster')['Category'].value_counts()
# print(cluster_summary)

# # Optional: View cluster composition
# df1['PCA1'] = X_pca[:, 0]
# df1['PCA2'] = X_pca[:, 1]
# # View the coordinates with Category and Cluster
# coords_df = df1[['Funds', 'Category', 'Cluster', 'PCA1', 'PCA2']]
# coords_df

import numpy as np
import pandas as pd
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Select only return columns
return_cols = [col for col in df1.columns if 'Return (%)' in col]
X_returns = df1[return_cols].replace('-', np.nan).astype(float)

# Step 2: Fill missing values with median
X_returns = X_returns.fillna(X_returns.median())

# Step 3: Scale the return features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_returns)

# Step 4: Apply Gaussian Mixture Model
n_clusters = 3
gmm = GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=42)
df1['GMM_Cluster'] = gmm.fit_predict(X_scaled)

# Step 5: Apply t-SNE for visualization
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)
df1['tSNE-1'] = X_tsne[:, 0]
df1['tSNE-2'] = X_tsne[:, 1]

# # Step 6: Plot t-SNE clusters
# plt.figure(figsize=(10, 6))
# sns.scatterplot(data=df1, x='tSNE-1', y='tSNE-2', hue='GMM_Cluster', palette='tab10', s=70)
# plt.title('GMM Clusters Based on Historical Returns (t-SNE)')
# plt.xlabel('t-SNE Component 1')
# plt.ylabel('t-SNE Component 2')
# plt.legend(title='Cluster')
# plt.grid(True)
# plt.tight_layout()
# plt.show()

# # Step 7: Print fund names in each cluster
# print("\nüîç Funds in each GMM Cluster:")
# for cluster_id in sorted(df1['GMM_Cluster'].unique()):
#     cluster_funds = df1[df1['GMM_Cluster'] == cluster_id]['Funds']
#     print(f"\nCluster {cluster_id} ({len(cluster_funds)} funds):")
#     print(cluster_funds.to_string(index=False))


# Step 8: Calculate average return and volatility for each cluster
cluster_summary = []

for cluster_id in sorted(df1['GMM_Cluster'].unique()):

    cluster_funds = df1[df1['GMM_Cluster'] == cluster_id]
    # avg_return = cluster_funds['Return (%)6 mo'].replace('-', np.nan).astype(float).mean()   # Mean of fund-wise average returns
    avg_return = cluster_funds['Return (%)1 yr'].replace('-', np.nan).astype(float).mean()
    cluster_data = X_returns[df1['GMM_Cluster'] == cluster_id]
    volatility = cluster_data.std(axis=1).mean()    # Mean of fund-wise std deviation

    cluster_summary.append({
        'Cluster': cluster_id,
        'Avg Return (%)': round(avg_return, 2),
        'Avg Volatility (%)': round(volatility, 2)
    })

# Convert to DataFrame
cluster_df = pd.DataFrame(cluster_summary)

# Step 9: Assign risk levels based on volatility thresholds (tweak as needed)
def assign_risk_tag(volatility):
    if volatility < 5:
        return 'Low Risk'
    elif volatility < 10:
        return 'Moderate Risk'
    else:
        return 'High Risk'

cluster_df['Risk Level'] = cluster_df['Avg Volatility (%)'].apply(assign_risk_tag)

# Display the cluster-wise summary
print("\nüìä Cluster-wise Risk Summary:")
print(cluster_df)

# Map cluster ID to risk tag
risk_map = dict(zip(cluster_df['Cluster'], cluster_df['Risk Level']))
df1['Risk Level'] = df1['GMM_Cluster'].map(risk_map)

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df1, x='tSNE-1', y='tSNE-2', hue='Risk Level', palette='Set2', s=70)
plt.title('t-SNE Plot Colored by Risk Level')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.legend(title='Risk Level')
plt.grid(True)
plt.tight_layout()
plt.show()

"""#Selecting a particular risk type"""

# Example user input
user_risk_preference = "Moderate Risk"

# Filter the dataframe for funds in the selected risk level
filtered_funds_df = df1[df1['Risk Level'] == user_risk_preference]

# Show the filtered results
print(f"\nüéØ Funds matching '{user_risk_preference}': {len(filtered_funds_df)} funds found.\n")
filtered_funds_df
# print(filtered_funds_df[['Funds', 'GMM_Cluster', 'Risk Level']].to_string(index=False))

filtered_df = filtered_funds_df
selected_schema_codes = filtered_df['Scheme Code'].tolist()
print(len(selected_schema_codes))

"""# Importing and Cleaning correlation dataset"""

# The CSV is assumed to have schema codes as both row index and column headers.
file_name = "Growth_Direct_MF_Corr.csv"
df_corr = pd.read_csv(file_name, index_col=0)

# Ensure the index and columns are strings (schema codes)
df_corr.index = df_corr.index.astype(int)
df_corr.columns = df_corr.columns.astype(int)
print(df_corr.shape[0], df_corr.shape[1])
print(df_corr.index.dtype)  # Check the index type
print(selected_schema_codes[0])  # Check the type of elements in selected_schema_codes

# Filter the matrix for schema codes (both rows and columns) acc to selected_schema_codes
# Find schema codes that exist in df_corr
valid_schema_codes = list(set(selected_schema_codes) & set(df_corr.index))

# Now filter using only valid schema codes
filtered_corr = df_corr.loc[valid_schema_codes, valid_schema_codes]
print(filtered_corr.shape)
corr_matrix = filtered_corr.to_numpy()  # Convert to numpy array for faster operations.

# Number of funds available after filtering
filtered_corr

funds_to_remove = filtered_corr.index[filtered_corr.isnull().any(axis=1)].tolist()
filtered_corr_cleaned = filtered_corr.drop(index=funds_to_remove, columns=funds_to_remove)
corr_matrix = filtered_corr_cleaned.to_numpy()
m = filtered_corr_cleaned.shape[0]
print(m)
filtered_corr_cleaned
filtered_funds = filtered_corr_cleaned.columns.tolist()
print(filtered_funds)

"""#Finding predicted return using ARIMA TimeSeries Model"""

import requests
from datetime import datetime
from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings

warnings.filterwarnings("ignore")

input_date = datetime(2026, 4, 30)
mu_preds = []

for scheme_code in filtered_funds:
    # 1. Fetch data
    data = requests.get(f"https://api.mfapi.in/mf/{int(scheme_code)}").json()["data"]
    df = pd.DataFrame(data)
    df["date"] = pd.to_datetime(df["date"], format="%d-%m-%Y")
    df["nav"] = pd.to_numeric(df["nav"], errors="coerce")
    df.set_index("date", inplace=True)

    # 2. Resample to month-end NAV
    monthly = df["nav"].resample("M").last().ffill()

    # 3. Calculate log-returns
    lr = np.log(monthly / monthly.shift(1)).dropna()

    # 4. Determine how many months ahead to forecast
    last_month = monthly.index.max()
    months_ahead = (input_date.year - last_month.year) * 12 + (input_date.month - last_month.month)
    if months_ahead <= 0:
        print(f"Scheme {scheme_code}: target date before last data point.")
        continue

    # 5. Fit SARIMAX on log-returns with annual seasonality
    try:
        model = SARIMAX(lr,
                        order=(1, 1, 1),
                        seasonal_order=(1, 1, 1, 12),
                        enforce_stationarity=False,
                        enforce_invertibility=False)
        fit = model.fit(disp=False)

        # 6. Forecast future log-returns and accumulate
        forecast_lr = fit.forecast(steps=months_ahead)
        cumulative_return = np.exp(forecast_lr.cumsum())[-1] - 1.0

        mu_preds.append((scheme_code, cumulative_return))
        print(f"[SARIMAX] Scheme {scheme_code}: Predicted {cumulative_return*100:.2f}% over {months_ahead} months")

    except Exception as e:
        print(f"Scheme {scheme_code}: modeling error: {e}")

# # Final results
# print("\nPredicted Returns:")
# for code, r in mu_preds:
#     print(f"  {code}: {r*100:.2f}%")

"""#Multi-objective optimization (Final 1)


"""

try:
  import pygad
except ImportError:
  !pip install pygad
  import pygad

import random

print(f"Risk profile is {user_risk_preference}")

# -------------------------------
# Problem Data and Parameters
# -------------------------------
# Assume mu_preds_arima is provided; for example:
# mu_preds_arima = [("118935", 0.001), ("140453", 0.001), ("119597", 0.0011), ("119659", 0.0011), ("119160", 0.0007), ...]
mu_preds_arima = [
    (scheme, pred)
    for scheme, pred in mu_preds
    if pred is not None and np.isfinite(pred)
]

scheme_codes = [scheme for scheme, _ in mu_preds_arima]
# print(len(scheme_codes))
mu_pred = np.array([pred for _, pred in mu_preds_arima])
N = mu_pred.shape[0]
n_select = 5  # Number of funds to select
print(f"Total funds used = {N}")
# Assume filtered_corr_cleaned is a DataFrame with scheme codes as both index and columns.
# For example, it might be a correlation or covariance matrix.
# print("Scheme Codes: ",type(scheme_codes[0]))
filtered_funds = filtered_corr_cleaned.columns.tolist()
# print("Filtered funds: ",type(filtered_funds[0]))
cov_matrix = filtered_corr_cleaned.loc[scheme_codes, scheme_codes].values

R_target=(cluster_df.loc[cluster_df['Risk Level'] == user_risk_preference, 'Avg Return (%)'].values[0])/100
print(f"Minimum Required Target = {R_target}")
PENALTY = 1e10
IND_SIZE = 2 * N

# -------------------------------
# Risk Profile and Lambda Setting
# -------------------------------

if user_risk_preference == "Low Risk":
    lambda1, lambda2 = 0.1, 0.9
elif user_risk_preference == "Moderate Risk":
    lambda1, lambda2 = 0.5, 0.5
elif user_risk_preference == "High Risk":
    lambda1, lambda2 = 0.9, 0.1
else:
    lambda1, lambda2 = 0.5, 0.5  # Default fallback
print(f"lambda1 and lambda2 are {lambda1} and {lambda2} respectively")

# -------------------------------
# Fitness Function
# -------------------------------
def fitness_wrapper(ga_instance, solution, solution_idx):
    # Split solution into selection vector (x) and weights (w)
    x = np.array(solution[:N]).astype(int)
    w = np.array(solution[N:])

    # Constraint 1: Exactly n_select funds must be selected
    if np.sum(x) != n_select:
        return -PENALTY

    # Constraint 2: Weights must sum to 1
    if abs(np.sum(w) - 1) > 0:
        return -PENALTY

    # Constraint 3: If a fund is not selected, its weight must be 0
    for i in range(N):
        if x[i] == 0 and w[i] > 0:
            return -PENALTY

    # Constraint 4: Portfolio expected return must be at least R_target
    expected_return = np.dot(w, mu_pred)
    if expected_return < R_target:
        return -PENALTY

    # Calculate average pairwise correlation among selected funds
    selected_indices = np.where(x == 1)[0]
    if len(selected_indices) < 2:
        avg_corr = 1.0
    else:
        sub_corr = cov_matrix[np.ix_(selected_indices, selected_indices)]
        sum_corr = np.sum(sub_corr) - np.trace(sub_corr)
        num_pairs = len(selected_indices) * (len(selected_indices) - 1)
        avg_corr = sum_corr / num_pairs

    # Calculate portfolio variance
    variance = np.dot(w, np.dot(cov_matrix, w))

    # Combined objective: weighted sum of variance and (optionally) avg_corr.
    # Here, since lambda1 and lambda2 sum the two objectives, lower is better.
    objective_value = lambda1 * avg_corr + lambda2 * variance
    fitness = -objective_value  # Negative because pygad maximizes fitness.
    return fitness

# -------------------------------
# Generate Valid Solution Function
# -------------------------------
def generate_valid_solution():
    solution = np.zeros(IND_SIZE, dtype=float)
    valid_indices = np.arange(N)
    # Select exactly n_select funds
    selected_indices = np.random.choice(valid_indices, size=n_select, replace=False)
    solution[selected_indices] = 1
    # Generate weights only for selected funds using Dirichlet distribution
    temp_weights = np.random.dirichlet(np.ones(n_select), size=1).flatten()
    full_weights = np.zeros(N)
    full_weights[selected_indices] = temp_weights
    solution[N:] = full_weights
    return solution

# -------------------------------
# PyGAD Configuration and Execution
# -------------------------------
population_size = 30
initial_population = [generate_valid_solution() for _ in range(population_size)]

ga_instance = pygad.GA(
    num_generations=10000,
    num_parents_mating=10,
    fitness_func=fitness_wrapper,
    sol_per_pop=population_size,
    num_genes=IND_SIZE,
    initial_population=initial_population,
    gene_type=float,
    gene_space=[0, 1],
    crossover_type="single_point",
    mutation_type="swap",
    mutation_percent_genes=10,
    stop_criteria="saturate_100",
    suppress_warnings=True,
    allow_duplicate_genes=False,
    random_seed=2
)

ga_instance.run()

# Get best solution
best_solution, best_solution_fitness, _ = ga_instance.best_solution()
x_best = best_solution[:N].astype(int)
w_best = best_solution[N:]
selected_funds_final = [scheme_codes[i] for i in np.where(x_best)[0]]

print("Selected funds:", selected_funds_final)
print("Optimal weights:", [f"{w:.4f}"for w in w_best if w > 0])
print("Expected return:", np.dot(w_best, mu_pred))
print("Portfolio variance:", np.dot(w_best, np.dot(cov_matrix, w_best)))
print("Combined Objective Value:", best_solution_fitness)

# 1. Extract the best fitness at each generation (PyGAD stores maximized fitness)
fitness_history = ga_instance.best_solutions_fitness  # Array of best fitness values
# 2. Convert fitness back to objective value (remember fitness = -objective)
objective_history = [-f for f in fitness_history]
plt.figure(figsize=(8, 5))
plt.plot(objective_history, linewidth=2)
plt.title("Genetic Algorithm Convergence")
plt.xlabel("Generation")
plt.ylabel("Objective Value (lower is better)")
plt.grid(True)
plt.tight_layout()
plt.show()

"""#Result Visualization"""

def visualize_portfolio_allocation(df, selected_funds, weights, title_suffix=""):
    """
    Visualizes portfolio allocation for selected funds.

    Parameters:
    - df: DataFrame containing fund metadata with columns ['Scheme Code', 'Funds', 'Fund Manager', 'Category',
           'Fund Type', 'Cluster']
    - selected_funds: list of Scheme Code integers
    - weights: list of floats corresponding to portfolio weights for selected_funds
    - title_suffix: optional string to append to the chart titles
    """
    # Filter and prepare the portfolio DataFrame
    portfolio_df = df[df['Scheme Code'].isin(selected_funds)].copy()
    portfolio_df = portfolio_df.drop_duplicates(subset='Scheme Code')
    portfolio_df['Weight'] = weights

    # Display final table
    display_df = portfolio_df[['Funds', 'Fund Manager', 'Category', 'Fund Type', 'Weight']]
    print("Final Portfolio Composition:\n")
    display_df.reset_index(drop=True, inplace=True)
    display(display_df)

    # Pie Chart 1: Fund Allocation
    plt.figure(figsize=(8, 6))
    plt.pie(portfolio_df['Weight'], labels=portfolio_df['Funds'], autopct='%1.1f%%', startangle=140)
    plt.title(f'Portfolio Allocation by Fund {title_suffix}')
    plt.axis('equal')
    plt.show()

    # # Pie Chart 2: Fund Type Split
    # fund_type_split = portfolio_df.groupby('Fund Type')['Weight'].sum()
    # plt.figure(figsize=(8, 6))
    # plt.pie(fund_type_split, labels=fund_type_split.index, autopct='%1.1f%%', startangle=140,
    #         colors=sns.color_palette('pastel'))
    # plt.title(f'Portfolio Split by Fund Type {title_suffix}')
    # plt.axis('equal')
    # plt.show()

    # Pie Chart 2: Equity vs Debt based on Cluster
    cluster_split = portfolio_df.groupby('Cluster')['Weight'].sum()
    equity_weight = cluster_split.get(1, 0)
    debt_weight = cluster_split.get(0, 0)
    plt.figure(figsize=(8, 6))
    labels = ['Equity (Cluster 1)', 'Debt (Cluster 0)']
    sizes = [equity_weight, debt_weight]
    explode = (0.05, 0.05)
    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, explode=explode, colors=['#4c72b0', '#c44e52'])
    plt.title(f'Equity vs Debt Allocation {title_suffix}')
    plt.axis('equal')
    plt.show()

"""High Risk"""

selected_funds_final = [119716, 119212, 119732, 119350, 133568]
w_best_final = [0.0554, 0.0475, 0.3812, 0.2569, 0.2589]
visualize_portfolio_allocation(df1, selected_funds_final, w_best_final, title_suffix="(High Risk)")

"""Moderate Risk"""

selected_funds_final = [120348, 120385, 120473, 119082, 115132]
w_best_final = [0.0686, 0.4448, 0.1112, 0.1450, 0.2304]
visualize_portfolio_allocation(df1, selected_funds_final, w_best_final, title_suffix="(Moderate Risk)")

selected_funds_final = [120401, 120482, 119625, 119779, 119800]
w_best_final = [0.2278, 0.0319, 0.2906, 0.1329, 0.3167]
visualize_portfolio_allocation(df1, selected_funds_final, w_best_final, title_suffix="(Low Risk)")